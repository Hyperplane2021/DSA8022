---
title: "Generalized Additive Models"
author: "GaryMcKeown"
date: "06/02/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gratia)
library(mgcv)
```

## Simple Linear Models

In understanding Generalized Additive Models it is best to start with a clear understanding of simple linear regression. Linear regression deals with models that are a straight line, this has the useful quality of being able to be described by a single number or parameter--in linear regression this is the beta coefficient.

The classic linear regression equation can be seen below

$$
y = \alpha + \beta_1x + \epsilon
$$
this is essentially the same as the classic equation for the slope with the edition of an extra term that captures error.

$$
y = mx  + c
$$

In the classic slope the equation $y$ is given by the value of $x$ times the $m$ value, $m$ defines the slope and is the single figure that multiples by $x$ to give the steepness of the slope. If $m$ is 1 then the slope will be at $45^\circ$ if it is less than 1 it will be lower than $45^\circ$ and if it is greater than 1 the slope will be greater than $45^\circ$. As can be seen in the following figure. Find and change the slope from 1 to less than 1 (e.g. 0.35) and then to more than 1 (e.g. 1.55). To get the subtitle to change you would have to adjust that too.

```{r}
theme_set(theme_grey(base_size=18))
ggplot(data=data.frame( x=c(1,2),y=c(1,2) ), aes(x=x,y=y)) + 
  #geom_point(shape = 1) +
  geom_abline(intercept = 0, slope = 1.55, col = "red") +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  labs(title = "y = mx + c", subtitle = "slope (m) = 1")
```

The $c$ in the slope equation represents a constant. This has the effect of shoving the line up or down the y axis, but does nothing to the angle of the slope. It is also the point where the line intercepts the y axis when x = zero. That is why it is called the intercept.

Play around with changing the intercept to get different values in the plot below.

```{r}
ggplot(data=data.frame( x=c(1,2),y=c(1,2) ), aes(x=x,y=y)) + 
  #geom_point(shape = 1) +
  geom_abline(intercept = 0.025, slope = 1, col = "red") +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  labs(title = "y = mx + c", subtitle = "intercept = 0.25")
```

The $y=mx+c$ formulation captures the model part of the regression equation well. However, in a real world data collection situation we never get data that falls along a straight line. We always have some aspects of the data that are not explained by the model. This is the error, the bits that are not captured by a line, the line of the linear model. The distance of each point from a model line is known as the residual for that point. It is the bit left over, the residual part not explained by the model.

If we take the toy data set generated by the code below we see change in a skill over time.

```{r}
Skill <- c(50.6, 51.5, 63.4, 49.4, 45.1, 51.3, 38.5, 51.9, 47.2, 59.8, 46.2, 45.7, 57.6, 53.9, 49.4, 59.5, 63.4, 64.0, 59.4, 61.4, 71.4, 57.3, 72.7, 70.0, 68.9)
Time <- c(1:25)
linearRegression1 <- data.frame(Time, Skill)
head(linearRegression1)
```

We can create the plot below which shows the data points and the best model (the regression line) drawn through it. the ggplot command geom_smooth() will put the regression model line through the data if we ask it to do using the method "lm"--for linear model. I have made this thicker using size = 2 as we will draw a line over the top of it. The coef() function gets the coefficients for the same model and we can see that  the intercept is 45.053 and the Beta coefficient is 0.8713077 and if we feed these into a geom_abline() command then we the same line, it is coloured green so that it can be seen over the previous regression line.


```{r}
print(coef(lm(Skill ~ Time, data = linearRegression1)))
ggplot(data=linearRegression1, aes(x=Time, y=Skill)) + 
  geom_point() + 
  geom_point(colour = "red") +
  geom_smooth(method = "lm", se = FALSE, size = 2) +
  geom_abline(intercept = 45.053, slope = 0.8713077, colour = "green")
```

The residuals in this line can be seen in the plot below

```{r}
model1 <- lm(Skill ~ Time, data = linearRegression1)
linearRegression1$predicted <- predict(model1)   # Save the predicted values
linearRegression1$residuals <- residuals(model1) # Save the residual values

ggplot(data=linearRegression1, aes(x=Time, y=Skill)) + 
  geom_point() + 
  geom_point(colour = "red") +
  geom_point(aes(y = predicted), shape = 1) +
  geom_segment(aes(xend = Time, yend = predicted), alpha = .5) +
  geom_text(aes(y = predicted+(residuals/2), label = paste0("", round(residuals, 1), "")), nudge_x = 0.45, size=2.5) + 
  #geom_abline(intercept = 45.053, slope = 0.8713077) + 
  geom_smooth(method = "lm", fill = NA)
```


## Non-linear Data
So far we have seen the basics of linear models. As stated linear models are great but they rely on a line being the best model that is not always the case. There are many times when a curve or a wiggly line is a better explanation of the relationship between two variables. We can see below what is known as Anscombe's quartet, these all have almost identical statistics. The lesson here is that we have to be wary when we just look at numbers and that we should graphically check our data to make sure we are not making a silly error. In plot 2 here we can see that the relationship is curvilinear, this is the kind of non-linear relationship that is capture better by Generalized Additive Models than linear models.

```{r}
anscombe_m <- data.frame()

for(i in 1:4)
  anscombe_m <- rbind(anscombe_m, data.frame(set=i, x=anscombe[,i], y=anscombe[,i+4]))

ggplot(anscombe_m, aes(x, y)) + 
  theme_bw(base_size=18) +
  geom_point(size=3, color="red", fill="orange", shape=21) + 
  geom_smooth(method="lm", fill=NA, fullrange=TRUE) + 
  facet_wrap(~set, ncol=2)
# from https://gist.github.com/amoeba/7576126
```

The code below creates a curve based on a sine wave using the sin() function, and adds some randomness to it using the rnorm() function. The sine wave would be the model we are trying to find in this data and the randomness that we added would be the error.

```{r}
set.seed(42)
x <- seq(0,2*pi,0.1)
z <- sin(x)
randomError <- rnorm(mean=0, sd=0.5*sd(z), n=length(x))
y <- z + randomError
curveData <- cbind.data.frame(x,y,z)
```

We can plot this data to have a look at it using ggplot, and put a linear regression line through it using geom_smooth() to get an idea of how inappropriate this would be. We can add the original model of the sine wave into this using geom_path() -- to do this remove the comments in the geom_path line and uncomment the plus on the previous line to ensure it is included in the plot. 

```{r}
ggplot(data=curveData, aes(x=x, y=y)) + 
  geom_point() + 
  geom_point(colour = "red") +
  geom_smooth(method = "lm", se = FALSE, size = 1) #+
  #geom_path(aes(x=x, y=z), linetype = "dashed") 
```


We can also look at the difference in the residuals in these two models. If we do the sum of squares of the residuals for this linear model we get 20.34016 (there is no need for this number of decimal places but I am keeping the output the same as R outputs it).

```{r}
model2 <- lm(y ~ x, data = curveData)
curveData$predicted <- predict(model2)   # Save the predicted values
curveData$residuals <- residuals(model2) # Save the residual values

ggplot(data=curveData, aes(x=x, y=y)) + 
  geom_point() + 
  geom_point(colour = "red") +
  geom_smooth(method = "lm", se = FALSE, size = 1) +
  #geom_path(aes(x=x, y=z), linetype = "dashed") 
  geom_point(aes(y = predicted), shape = 1) +
  geom_segment(aes(xend = x, yend = predicted), alpha = .5) +
  geom_text(aes(y = predicted+(residuals/2), label = paste0("", round(residuals, 1), "")), size=2.5)

  print(sum(curveData$residuals^2)) # the sum of squares of the residuals 
  #summary(model2)
  #anova(model2)
```

As we know the exact model before we added the randomness we can do the same thing and get the sum of squares from the exact model. We can see this time the sum of squares of the residuals is 9.975666 much smaller indicating that the true model fits the data better as we might expect. Also not surprisingly if we do the sum of squares of the randomError variable we created earlier we see it is the same number. Remove the comment in the last line to check this.

```{r}
model2 <- lm(y ~ x, data = curveData)
curveData$predicted <- predict(model2)   # Save the predicted values
curveData$residuals <- residuals(model2) # Save the residual values

ggplot(data=curveData, aes(x=x, y=y)) + 
  geom_point() + 
  geom_point(colour = "red") +
  #geom_smooth(method = "lm", se = FALSE, size = 1) +
  geom_path(aes(x=x, y=z), linetype = "dashed") +
  geom_point(aes(y = z), shape = 1) +
  geom_segment(aes(xend = x, yend = z), alpha = .5) +
  geom_text(aes(y = z+((y-z)/2), label = paste0("", round((y-z), 1), "")), size=2.5)

  print(sum((y-z)^2)) # the sum of squares of the residuals against the true model, the sine wave
  #sum(randomError^2)
```

As yet we have not fitted a GAM model to see if it does better than the linear model. We can do a quick cheat to get ourselves a smooth as ggplot has a smooth function built in. Changing the geom_smooth method to auto chooses a loess method if there are less than 1000 observations or a GAM if there are more than 1000 observations. Here we see a loess smooth, which stands for locally estimated scatterplot smoothing and is a topic you can look up yourselves--it provides a smooth but is not quite as sophisticated as a generalized additive model.

```{r}
ggplot(data=curveData, aes(x=x, y=y)) + 
  geom_point() + 
  geom_point(colour = "red") +
  geom_smooth(method = "auto", se = FALSE, size = 1)
```

## Generalized Additive Models

The package that is best to use for GAMs is mgcv by Simon Woods and he has a great book to accompany it. Also recommended are the various books on GAMs by Alain Zuur. We can load this library and create some model data using the code below. The first time we run this section it will load mgcv which will then go on to load other packages it uses such as nlme, a mixed modelling regression package.

```{r}
library(mgcv)
```

The difference between a GAM and a linear model is that a GAM does not capture the a line it looks for a nonlinear explanation. As a consequence the model can no longer be specified with a single parameter, but needs a mathematical function to explain the model and the process of finding that function is what the GAM methods do. As a result the equation for a basic GAM is as follows

$$
y = \alpha + f(x) + \epsilon
$$
Where the $f()$ represents the function.

GAMs in R are created in a very similar way to linear models. They have the same formula structure with the difference that the GAM contains an s() bit around the predictor variable. This s() means spline which is a mathematical way of drawing curves. The below code runs the linear model and the GAM and prints out the summary of each.

```{r}
model2 <- lm(y ~ x, data = curveData)
summary(model2)
print("-----------------------------")
model3 <- gam(y ~ s(x), data = curveData, method = "REML")
summary(model3)
```

We can simply plot the GAM to see how well it did in estimating our true model. This looks fairly good as an estimation. 

In this plot command residuals = TRUE tells the plot to include the data points of the residuals (these are partial residuals), se = FALSE turns off the variability bands (these are 95% confidence intervals determine in a pointwise way), rug = FALSE turns off the data points being placed on the x axis, pch chooses the shape of the points and cex chooses their size. PLay around with the values of these variables to see what they do. Try the other arguments that are commented out.

```{r}
plot(model3, residuals = TRUE, se=TRUE, rug=FALSE, pch = 1, cex = 0.5) # shade = TRUE, shade.col = "rosybrown2"
```


We can look at the model in the way that we did earlier in a ggplot version by extracting the predicted parts of the model.If we do the sum of squares of the residuals in both this model and against the true model we can see that the true model is giving us 9.975666 while the GAM is giving us 8.796335, an even smaller number. This is much better than the 20.34016 we got with the linear model, but is it too good. That the sum of squares of the residuals is smaller than the true model suggests that the GAM may be closer to the data than we would like--it is slightly overfitted but still doing a good job.

```{r}
curveData$predicted <- predict(model3)   # Save the predicted values
curveData$residuals <- residuals(model3) # Save the residual values

ggplot(data=curveData, aes(x=x, y=y)) + 
  geom_point() + 
  geom_point(colour = "red") +
  #geom_smooth(method = "lm", se = FALSE, size = 1) +
  geom_path(aes(x=x, y=predicted), colour = "blue") +
  geom_point(aes(y = predicted), shape = 1) +
  geom_segment(aes(xend = x, yend = predicted), alpha = .5) +
  geom_text(aes(y = predicted+(residuals/2), label = paste0("", round(residuals, 1), "")), size=2.5)

  print(sum((y-z)^2)) # the sum of squares of the residuals against the true model, the sine wave
  print(sum(curveData$residuals^2)) # the sum of squares of the GAM residuals
```

A GAM smooth is actually made up of a lot of coefficients multiplied by basis functions.

As with a linear model we can see the coefficients by using the coef() function.

```{r}
coef(model3)
```

Here we can see that this model has 9 coefficients and an intercept parameter. We can also see the basis functions that are multiplied by these basis functions to make the smooth. We can do this using the function from the gratia package written by Gavin Simpson whose blog (https://www.fromthebottomoftheheap.net/blog/) contains lots of great advice on GAMs and other statistics.
gratia was written to bring the worlds of the tidyverse and the GAMs from mgcv together.

```{r}
bs <- basis(s(x), data = curveData)
draw(bs)
```


We can run some diagnostics on the GAM model to see how well it is doing with the gam.check() function. The fourth output here shows us the textual model. 

```{r}
gam.check(model3)
```

We can also use the gratia version of gam.check to check the number of knots bring used is sufficient - k.check().

```{r}
k.check(model3)
```

and the appraise() function to get nicer plots made with ggplot.

```{r}
appraise(model3)
```

gratia will also allow us to produce ggplots of the smooth more easily than the ones I did earlier.

```{r}
draw(model3, rug = FALSE)
```


One of the important elements of a GAM is how wiggly the smooth is allowed to be. This is determined by the number of joining knots that are allowed in a spline. In essence a spline is a piecewise combination of lots of little cubic sections. The number of knots shows how much wiggle there can be in the smooth. We can determine this by adding a k parameter to our spline model. In the previous GAM model we just let the model decide for itself and we can use the obscure command model4$smooth[[1]]$bs.dim to extract the basis dimensions from the model 

```{r}
model3$smooth[[1]]$bs.dim
```
This tells us that it used 10 as a default. IN the model below the k parameter has been add. Play around with various values of k and see what it does to the plot and to the sum of squares values as you increase or decrease the value. What happens with the values 15, 8, 5, 3

```{r}
model4 <- gam(y ~ s(x, k=7), data = curveData, method = "REML")
summary(model4)

curveData$predicted <- predict(model4)   # Save the predicted values
curveData$residuals <- residuals(model4) # Save the residual values

ggplot(data=curveData, aes(x=x, y=y)) + 
  geom_point() + 
  geom_point(colour = "red") +
  #geom_smooth(method = "lm", se = FALSE, size = 1) +
  geom_path(aes(x=x, y=predicted), colour = "blue") +
  geom_point(aes(y = predicted), shape = 1) +
  geom_segment(aes(xend = x, yend = predicted), alpha = .5) +
  geom_text(aes(y = predicted+(residuals/2), label = paste0("", round(residuals, 1), "")), size=2.5)

  print(sum((y-z)^2)) # the sum of squares of the residuals against the true model, the sine wave
  print(sum(curveData$residuals^2)) # the sum of squares of the residuals against the true model, the sine wave
  #sum(randomError^2)
```

Three is an interesting one because it basically turns the smooth into the linear model that we saw before there are not really enough wiggliness to capture the model with only three knots, so a linear model is the best it can do. Somewhere around 6 is probably the best number given that we know the true model.

## Spatial GAMs

I will not spend time on this but GAMs are also very good at dealing with spatial information in two dimensions. You can see that from one of the simuated gam functions that come with the gam package used in conjunction with the gratia package. Try chainging the seed and running it a few times. You will notice that it takes a little time to run this, and it should be noted that GAMs and especially their mixed model evivalent GAMMs (Generalized Additive Mixed Models) are quite computationally intensive.

```{r}
set.seed(4)
dat <- gamSim(2, n = 4000, dist = "normal", scale = 1, verbose = FALSE)
mod <- gam(y ~ s(x, z, k = 30), data = dat$data, method = "REML")
draw(mod)
```

